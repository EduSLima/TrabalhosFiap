---
title: "Análise Qualidade do Vinho"
author: "FIAP-06IA"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    self_contained: true
    thumbnails: false
    lightbox: false
---


```{r knitr_init, echo=FALSE, results="asis", cache=FALSE}
library(rmarkdown)
library(knitr)
library(rmdformats)
library(DT)

## Global options
options(max.print = "75")
opts_chunk$set(echo = FALSE,
	             cache = FALSE,
               prompt = FALSE,
               tidy = FALSE,
               comment = NA,
               message = FALSE,
               warning = FALSE)
opts_knit$set(width = 75)
```

# Início

## Introdução

<img src="Arquivos/vinho.jpg" style="width:25%; border:10px solid; margin-right: 20px" align="left">

>Trabalho de conlusão </br>
>Matéria   : CONCEITOS ESTATÍSTICOS PARA IA </br>
>Professora: ADELAIDE ALVES DE OLIVEIRA </br>

</br>

Integrantes do Grupo E2GR:

* EDUARDO MORAIS           [ 334530 ]
+ EDUARDO SIQUEIRA DE LIMA [ 334304 ]
+ GABRIEL SHIKAMA          [ 334068 ]
+ RICARDO CALIMANIS        [ 334759 ]

Análise realizada no dataset WineQuality onde aplicaremos todas as técnicas aprendidas em sala de aula.


## Preparando o Ambiente

### Instalando os pacotes necessários


Instalando os pacotes necessários para realizar as análises 
```{r chunk="idx_01_01", echo=TRUE, eval=TRUE}

#lista de pacotes que iremos utilizar no projeto
Pacotes_Necessarios <- c("ggplot2","readr","dplyr","corrplot","plotly","skimr","GGally","gmodels","ggpubr","caTools",
                         "caret" ,"rpart.plot","DT","e1071","corrgram")

#com base nos pacotes instalados crio uma variavel somente com os pacotes 
#que não temos ainda para realizar a instalação 
#dos pacotes que de fato não possuimos
PacotesNovos <- Pacotes_Necessarios[!(Pacotes_Necessarios %in% installed.packages()[,"Package"])]
if(length(PacotesNovos)){ install.packages(PacotesNovos)} else {print("Todos os Pacotes Estão Instalados")}


```

### Carregando os Pacotes

```{r chunk="idx_01_02", echo=TRUE, eval=TRUE}
lapply(Pacotes_Necessarios, require, character.only = TRUE)
```

## Dataset

### Carregando o DataSet

A fim de facilitar a compreensão e desenvolvimento de nosso codigo decidimos mudar os nomes das colunas do data set, a tabela a baixo indica o nome original do arquivo e o nome que propuzemos

### Lista DE - PARA das colunas

Nome no Arquivo     |  Nome Traduzido
----------------    |-----------------
ID                  |ID (que não sera utilizado)
fixed acidity       |acidez_fixa
volatile acidity    |acidez_volatil
citric acid         |acido_citrico	
residual sugar      |acucar_residual
chlorides           |cloretos
free sulfur dioxide |fsd
total sulfur dioxide|tsd
density             |densidade
pH                  |PH
sulphates           |sulfatos
alcohol             |grau_alcolico
quality             |qualidade
Vinho               |Tipo

```{r chunk="idx_01_03" , echo=TRUE, eval=TRUE}
#Criando uma variável nome_colunas que receberá os nomes das colunas que normalizaremos a fim de facilitar o resto da análise
nome_colunas <- c("id","acidez_fixa","acidez_volatil","acido_citrico","acucar_residual","cloretos", "fsd", "tsd","densidade","PH", 
                  "sulfatos","grau_alcolico","qualidade","tipo")

#uso da biblioteca readr é para obter uma performance de carga melhor que a lib padrão do R
#e escolhemos o read_csv2 justamente pelo fato do arquivo estar separado por ; ao invés de ,
#o separador decimal também não é o . que é convencional e este comando ja os converte facilmente
#skip = 1 para ignorar o cabecalho que mudamos para melhor entendimento

setwd("/mnt/hgfs/kal1s/files/cyberAI/Training/Fiap/MBA/Disciplinas/TrabalhosFiap/EstatisticaIA/Final")
vinhos <- read_csv2("Arquivos/BaseWine_Red_e_White.csv" ,col_names = nome_colunas, skip = 1)

```

# Análise Exploratória

## Observando o DataFrame

Exibindo as Dimensões do dataframe vinhos
```{r chunk="idx_02_01" , echo=TRUE, eval=TRUE}

dim(vinhos)

```


Exibindo a Estrutura do dataframe vinhos
```{r chunk="idx_02_02" , echo=TRUE, eval=TRUE}

str(vinhos)

```

## Validações Iniciais

Exibindo a Sumario e um histograma inicial  do dataframe vinhos

```{r chunk="idx_02_03" , echo=TRUE, eval=TRUE}
options(width = 900)   #definindo o tamanho da area de impressão de saida do markdown
options(max.print=500) # aumentando a saida da lista, sem esta opção alguns resultados tendem a ser cortados
skim(vinhos[, names(vinhos) != "id"] ) #retirando a coluna ID da análise

```

Observa-se que:

* O campo `acucar_residual`,`fsd`, `tsd` possuem um desvio padrão acima das demais variaveis
* A maioria dos histogramas apresenta uma distribuição normal entretanto não centralizado o que pode indicar a presença de outliers


### Checar presença de `nulos`
```{r chunk="idx_02_04" , echo=TRUE, eval=TRUE}
sapply(vinhos, function(x)all(is.na(x)))
```

O resultado acima nos descreve que não há presença de nulos na base, isto é indicado pelo retorno `FALSE` em cada variável

### Checar presença de Registros Duplicados
 
  Para prover melhor performance e acurácia de nossos modelos iremos verificar a existência de registros duplicados e removê-los se existir, mais antes é necessário remover uma coluna, ou simplesmente ignorar, que é a coluna ID que contem algum tipo de código incremental.

```{r chunk="idx_02_05" , echo=TRUE, eval=TRUE}

#removendo a coluna Id que não é necessária para nossa analise
vinhos<- vinhos[-1]


vinhos[duplicated(vinhos, fromLast = TRUE), ]

```

De fato existem 1176 registros duplicados onde:
```{r chunk="idx_02_06" , echo=TRUE, eval=TRUE}
count(vinhos[duplicated(vinhos, fromLast = TRUE), ], tipo)
```

Removendo as linhas duplicadas
```{r chunk="idx_02_07" , echo=TRUE, eval=TRUE}

vinhos<-vinhos[!duplicated(vinhos[-1], fromLast = TRUE), ]

```

### Histograma

Imprimindo Histogramas das variaveis
```{r chunk="idx_02_08" , echo=TRUE, eval=TRUE}
attach(vinhos)

Rotulos_Colunas <-c("acidez_fixa","acidez volatil"	,"acido citrico","acucar residual","cloretos","fsd","tsd","densidade",			
                    "PH","sulfatos","grau alcolico","qualidade","tipo")

grafico_lista <- vector("list", length = length(Rotulos_Colunas)-2)

for(i in 2:13){
  grafico_lista[[i-1]] <- plot_ly(x = as.formula(vinhos[i]),   type = 'histogram', name = Rotulos_Colunas[i])
}  
subplot(grafico_lista,  nrows = 4)


```


Podemos observar que quase em todas variáveis possuem um desenho ser similar à uma distribuição normal no entanto isso se deu pois mais à esquerda exceto grau alcoolico. Isso pode indicar presença de Outliers. A Conslusão que já se pode tirar é que há erros no Teor alcoolico, haja visto que é sabido que não existe vinhos com teor alcoolico a baixo de 8. 


## Explorando o DataSet

  Nosso intuito nesta parte é entender se podemos considerar o dataset como um todo ou se devemos observá-los por tipo de vinho para isso iremos agregar os dados por tipo de vinho 
  e ver como as variáveis se comportam

```{r chunk="idx_02_12" , echo=TRUE, eval=TRUE}
  
aggregate(vinhos[,-12],  by = list(vinhos$tipo),  FUN = sd)

```

Nos parece que há algumas diferenças significativas levando em consideração, os desvios padrão agregado por tipo de vinho onde:

  | Observações
--|-------------------------------------
1 | Acidez fixa é quase o dobro em vinhos `Tintos`
2 | Acidez Volatil é maior 0.7 desvios em `Tintos`
3 | Ácido Cítrico é quase 4 desvios maior em `Brancos`
4 | Cloretos maior que 2 desvios em `Tintos`
5 | Sulfatos (fsd e tsd) é Maior em `Brancos`
6 | Densidade é maior em  `Brancos`

Entretanto a Qualidade não varia, ou seja em nossa perceção as características que determinam qualidade para os vinhos são diferentes e iremos ver a seguir a correlação dessas variáveis.


# Preparação dos Dados

## Técnica

### Transformação de qualidade em variável categórica

Decidimos por classificar a nota da qualidade inicialmente em três grupos:

Grupo   | Notas
--------|----------
Ruim    | 0 ~ 5.99
Regular | 6 ~ 7.99
Bom     | >= 8

Neste caso, poderíamos utilizar algoritmos supervisionados como o K-means pra predizer em qual categoria um vinho se encontra.

Porém, consideramos que isso não faria sentido para rodar os modelos não supervisionados.

Para rodar este modelo, decidimos criar a variável `GrupoQualidade`, sendo qualquer `qualidade` com valor maior ou superior a 6 é classificado como vinho "BOM". A variável `GrupoQualidade` será nossa variável dependente no caso.

```{r chunk="idx_03_01" , echo=TRUE, eval=TRUE}

vinhos$GrupoQualidade  <- as.factor(ifelse(vinhos$qualidade > 6,1,0))
vinhos$GrupoQualidadeF <- as.factor(ifelse(vinhos$qualidade > 'Bom','Regular','Ruim'))

```


  Como Identificamos que pode fazer sentido analisar o vinho de maneira separada por tipo já que muitas variáveis tendem a se comportar de forma diferente vamos iniciar a preparação dos
dados separando o dataset em 2 :
`df_base_tinto` e `df_base_branco`

```{r chunk="idx_03_02" , echo=TRUE, eval=TRUE}
  
  df_base_tinto  <-as.data.frame(subset(vinhos[,1:15], tipo=="RED"))
  df_base_branco <-as.data.frame(subset(vinhos[,1:15], tipo!="RED"))
  
```


## Transformação Box Cox 

  Em estatística, uma transformação de potência é uma família de funções que são aplicadas para criar a transformação monotônica de dados usando funções de potência. Esta é uma técnica de transformação de dados útil usada para estabilizar a variância, tornar os dados mais semelhantes à distribuição normal, melhorar a validade das medidas de associação (como a correlação de Pearson entre as variáveis) e para outros procedimentos de estabilização de dados.

  Tanto a forma linear quanto a logarítmica são dois casos particulares de uma família mais extensa de transformações não-lineares. A transformação de potência é definida como uma função de variação contínua, em relação ao parâmetro de potência ?? (lambda), ou seja, x??. Uma classe geral de transformação que pode ser utilizada é a de Box-Cox, definida por:

para ?? diferente de 0
$$f_\lambda(x) = \frac{(x)^\lambda - 1}{\lambda} $$ 

para ?? = 0
$$f_0 = log(x)$$ 

Se a assimetria for 0, os dados são perfeitamente simétricos.
Como regra geral: Se a assimetria for menor que -1 ou maior que 1, a distribuição é muito distorcida.
Se a assimetria estiver entre -0,5 e 0,5, a distribuição é aproximadamente simétrica.

Usamos a transformação Boxcox e transformamos os dados e depois verificaremos a  assimetria.

### Vinho Tinto

Antes de transformar
```{r chunk="idx_03_01" , echo=TRUE, eval=TRUE}
apply(df_base_tinto[1:12], 2, skewness, na.rm =TRUE)
```

Transformado

```{r chunk="idx_03_02" , echo=TRUE, eval=TRUE}

#preparação para a transformação dos dados
df_prep_tinto <- preProcess(df_base_tinto[,1:12], c("BoxCox", "center", "scale"))
df_tinto <- data.frame(trans = predict(df_prep_tinto, df_base_tinto))

df_tinto

#remove df desnecessario
rm("df_prep_tinto")

#atribui os nomes originais 
colnames(df_tinto) <- colnames(df_base_tinto)

apply(df_tinto[1:12], 2, skewness, na.rm =TRUE)
```

### Vinho Branco

Antes de transformar
```{r chunk="idx_03_02" , echo=TRUE, eval=TRUE}
apply(df_base_branco[1:12], 2, skewness, na.rm =TRUE)
```

Transformado

```{r chunk="idx_03_03" , echo=TRUE, eval=TRUE}

#preparação para a transformação dos dados
df_prep_branco <- preProcess(df_base_branco[,1:12], c("BoxCox", "center", "scale"))
df_branco <- data.frame(trans = predict(df_prep_branco, df_base_branco))

#atribui os nomes originais 
colnames(df_branco) <- colnames(df_base_branco)

#remove df desnecessario
rm("df_prep_branco")


apply(df_branco[1:12], 2, skewness, na.rm =TRUE)
```


## Outliers

A maioria das estatísticas paramétricas, como médias, desvios-padrão e correlações, e todas as estatísticas com base nelas, são altamente sensíveis a outliers. As premissas dos procedimentos estatísticos comuns, como regressão linear e ANOVA, também são baseadas nessas estatísticas, quando outliers podem perturbar a estatística. análise. Assim, nós removemos os outliers.

Possivelmente, o passo mais importante na preparação de dados é identificar outliers. Como se trata de dados multivariados, consideramos apenas aqueles pontos que não possuem nenhum valor de variável de previsão para estar fora dos limites construídos pelos boxplots. A seguinte regra é aplicada:

Um valor preditivo é considerado um valor discrepante somente se for maior que 3 Desvios Padrão. A lógica por trás dessa regra é que os valores extremos extremos estão todos na extremidade superior dos valores e as distribuições são todas positivamente distorcidas.

### Vinho Tinto

#### Identificando os Outliers

Iremos a seguir criar um dataframe somente para ter a quantidade de outliers identificados para cada variável, usaremos o comando abs para obter a posicão absoluta
onde o desvio padrão é > 3 como ja fora transformado no passo anterior

```{r chunk="idx_03_04" , echo=TRUE, eval=TRUE}

outlier <- data.frame(matrix(ncol = 1, nrow = 1))
colnames(outlier)<-"tipo"

outlier$tipo = "Tinto"

outlier$acidez_fixa <- count(df_tinto[abs(df_tinto$acidez_fixa)>3,])
outlier$acidez_volatil <-count(df_tinto[abs(df_tinto$acidez_volatil)>3,])
outlier$acido_citrico <-count(df_tinto[abs(df_tinto$acido_citrico)>3,])
outlier$acucar_residual <-count(df_tinto[abs(df_tinto$acucar_residual)>3,])
outlier$cloretos <-count(df_tinto[abs(df_tinto$cloretos)>3,])
outlier$fsd <-count(df_tinto[abs(df_tinto$fsd)>3,])
outlier$tsd <-count(df_tinto[abs(df_tinto$tsd)>3,])
outlier$densidade <-count(df_tinto[abs(df_tinto$densidade)>3,])
outlier$PH <-count(df_tinto[abs(df_tinto$PH)>3,])
outlier$sulfatos <-count(df_tinto[abs(df_tinto$sulfatos)>3,])
outlier$grau_alcolico <-count(df_tinto[abs(df_tinto$grau_alcolico)>3,])

summary(outlier)

```

Encontramos 67 observações e iremos remover de nossa análise

#### Removendo os outliers


```{r chunk="idx_03_04" , echo=TRUE, eval=TRUE}

df_tinto <- df_tinto[!abs(df_tinto$acidez_fixa)>3,]
df_tinto <- df_tinto[!abs(df_tinto$acidez_volatil)>3,]
df_tinto <- df_tinto[!abs(df_tinto$acido_citrico)>3,]
df_tinto <- df_tinto[!abs(df_tinto$acucar_residual)>3,]
df_tinto <- df_tinto[!abs(df_tinto$cloretos)>3,]
df_tinto <- df_tinto[!abs(df_tinto$fsd)>3,]
df_tinto <- df_tinto[!abs(df_tinto$densidade)>3,]
df_tinto <- df_tinto[!abs(df_tinto$PH)>3,]
df_tinto <- df_tinto[!abs(df_tinto$sulfatos)>3,]
df_tinto <- df_tinto[!abs(df_tinto$grau_alcolico)>3,]

```




#### Validando o Dado {.tabset .tabset-fade}

É possivel notar que após a remoção dos outliers os dados se encontram mais normalizados

##### Antes  

```{r chunk="idx_03_05" , echo=TRUE, eval=TRUE}
attach(df_base_tinto)

Rotulos_Colunas <-c( "acidez_fixa","acidez volatil"	,"acido citrico","acucar residual","cloretos","fsd","tsd","densidade",			
                    "PH","sulfatos","grau alcolico","qualidade")

p_1 <- vector("list", length = length(Rotulos_Colunas)-2)

for(i in 2:12){
  p_1[[i-1]] <- plot_ly(x = as.formula(df_base_tinto[i]),   type = 'histogram', name = Rotulos_Colunas[i])
}  

subplot(p_1,  nrows = 4)
```

##### Depois

```{r chunk="idx_03_06" , echo=TRUE, eval=TRUE}
attach(df_tinto)

p_2 <- vector("list", length = length(Rotulos_Colunas)-2)

for(i in 2:11){
  p_2[[i-1]] <- plot_ly(x = as.formula(df_tinto[i]),   type = 'histogram', name = Rotulos_Colunas[i])
}  

subplot(p_2,  nrows = 4)

```



### Vinho Branco


```{r chunk="idx_03_07" , echo=TRUE, eval=TRUE}

outlier <- data.frame(matrix(ncol = 1, nrow = 1))
colnames(outlier)<-"tipo"

outlier$tipo = "Branco"

outlier$acidez_fixa <- count(df_branco[abs(df_branco$acidez_fixa)>3,])
outlier$acidez_volatil <-count(df_branco[abs(df_branco$acidez_volatil)>3,])
outlier$acido_citrico <-count(df_branco[abs(df_branco$acido_citrico)>3,])
outlier$acucar_residual <-count(df_branco[abs(df_branco$acucar_residual)>3,])
outlier$cloretos <-count(df_branco[abs(df_branco$cloretos)>3,])
outlier$fsd <-count(df_branco[abs(df_branco$fsd)>3,])
outlier$tsd <-count(df_branco[abs(df_branco$tsd)>3,])
outlier$densidade <-count(df_branco[abs(df_branco$densidade)>3,])
outlier$PH <-count(df_branco[abs(df_branco$PH)>3,])
outlier$sulfatos <-count(df_branco[abs(df_branco$sulfatos)>3,])
outlier$grau_alcolico <-count(df_branco[abs(df_branco$grau_alcolico)>3,])

summary(outlier)

```

Encontramos 153 observações fora do padrão e iremos remover de nossa análise

#### Removendo os outliers


```{r chunk="idx_03_08" , echo=TRUE, eval=TRUE}

df_branco <- df_branco[!abs(df_branco$acidez_fixa)>3,]
df_branco <- df_branco[!abs(df_branco$acidez_volatil)>3,]
df_branco <- df_branco[!abs(df_branco$acido_citrico)>3,]
df_branco <- df_branco[!abs(df_branco$cloretos)>3,]
df_branco <- df_branco[!abs(df_branco$fsd)>3,]
df_branco <- df_branco[!abs(df_branco$tsd)>3,]
df_branco <- df_branco[!abs(df_branco$densidade)>3,]
df_branco <- df_branco[!abs(df_branco$PH)>3,]
df_branco <- df_branco[!abs(df_branco$sulfatos)>3,]

```




#### Validando o Dado {.tabset .tabset-fade}

Assim como no vinho tinto, temos a mesma percepção de com poucas extrações o dado ficou também mais proximo do normal 

##### Antes  

```{r chunk="idx_03_05" , echo=TRUE, eval=TRUE}
attach(df_base_branco)

Rotulos_Colunas <-c("id", "acidez_fixa","acidez volatil"	,"acido citrico","acucar residual","cloretos","fsd","tsd","densidade",			
                    "PH","sulfatos","grau alcolico","qualidade","tipo")

p_1 <- vector("list", length = length(Rotulos_Colunas)-2)

for(i in 2:11){
  p_1[[i-1]] <- plot_ly(x = as.formula(df_base_branco[i]),   type = 'histogram', name = Rotulos_Colunas[i])
}  

subplot(p_1,  nrows = 4)
```

##### Depois

```{r chunk="idx_03_06" , echo=TRUE, eval=TRUE}
attach(df_branco)

p_2 <- vector("list", length = length(Rotulos_Colunas)-2)

for(i in 2:11){
  p_2[[i-1]] <- plot_ly(x = as.formula(df_branco[i]),   type = 'histogram', name = Rotulos_Colunas[i])
}  

subplot(p_2,  nrows = 4)

```


# Matriz de Correlação

## Técnica

  O primeiro passo para iniciar o processo de aplicação de modelos estatísticos é validar as correlações que possam indicar caracteristicas que determinam o fato.E neste caso precisamos saber quais das variáveis podem agregar qualidade ao vinho.
  

## Vinho Tinto 

```{r chunk="idx_03_07" , echo=TRUE, eval=TRUE}

corrplot(cor(df_tinto[1:12]), type = "lower")
```  
 
```{r chunk="idx_03_08" , echo=TRUE, eval=TRUE}

corrgram(df_tinto[1:12], type="data", lower.panel=panel.conf, 
         upper.panel=panel.shade, main= "Correlações para Vinho Tinto", order=T, cex.labels=1.1)
```   


## Vinho Branco  

```{r chunk="idx_03_07" , echo=TRUE, eval=TRUE}

corrplot(cor(df_branco[1:12]), type = "lower")
```  
 
```{r chunk="idx_03_08" , echo=TRUE, eval=TRUE}

corrgram(df_branco, type="data", lower.panel=panel.conf, 
         upper.panel=panel.shade, main= "Correlações para Vinho Branco", order=T, cex.labels=1.1)
```   

# Regressão Linear

## Técnica
  Regressão linear é uma equação para se estimar a condicional (valor esperado) de uma variável y, dados os valores de algumas outras variáveis x.

  Exemplo de regressão linear.
    A regressão, em geral, tem como objectivo tratar de um valor que não se consegue estimar inicialmente.

  A regressão linear é chamada "linear" porque se considera que a relação da resposta às variáveis é uma função linear de alguns parâmetros. Os modelos de regressão que não são uma função linear dos parâmetros se chamam modelos de regressão não-linear. Sendo uma das primeiras formas de análise regressiva a ser estudada rigorosamente, e usada extensamente em aplicações práticas. Isso acontece porque modelos que dependem de forma linear dos seus parâmetros desconhecidos, são mais fáceis de ajustar que os modelos não-lineares aos seus parâmetros, e porque as propriedades estatísticas dos estimadores resultantes são fáceis de determinar.[1]

### Formula 

  $$y_{i} = \alpha + \beta X_{i} + \varepsilon_{i}$$

onde:
  
$$y_{i}$$: Variável explicada (dependente); representa o que o modelo tentará prever
$$\alpha$$: É uma constante, que representa a interceptação da reta com o eixo vertical;
$$\beta$$: Representa a inclinação (coeficiente angular) em relação à variável explicativa;
$$X_{i}$$: Variável explicativa (independente);
$$\varepsilon _{i}}$$: Representa todos os factores residuais mais os possíveis erros de medição. O seu comportamento é aleatório, devido à natureza dos factores que encerra. Para que essa fórmula possa ser aplicada, os erros devem satisfazer determinadas hipóteses, que são: terem distribuição normal, com a mesma variância independentes e independentes da variável explicativa X, ou seja, i.i.d. (independentes e identicamente distribuídas).

fonte: https://pt.wikipedia.org/wiki/Regress%C3%A3o_linear


## Vinho Tinto

### Separação dos Dados Treino e Teste

```{r chunk="idx_04_01" , echo=TRUE, eval=TRUE}
set.seed(1914) #seed relacionado ao ano de fundação do nosso amado e glorioso Palestra Itália

split <- sample.split(df_tinto$qualidade, SplitRatio = 0.8)

#dividindo o dataset para treino e teste
df_vinho_tinto_treino <- subset(df_tinto, split == TRUE)
df_vinho_tinto_teste  <- subset(df_tinto, split == FALSE)


```


### Modelo Vinho Tinto

```{r chunk="idx_04_02" , echo=TRUE, eval=TRUE}

Modelo_01 <- lm(qualidade~ acidez_fixa+acidez_volatil+acido_citrico+acucar_residual+cloretos+densidade+fsd+grau_alcolico+PH+sulfatos+tsd, 
                data = df_vinho_tinto_treino)

precisao.tinto.rlinear = summary(Modelo_01)$r.squared * 100

summary(Modelo_01)
plot(Modelo_01)
```

### Treino do Modelo

```{r chunk="idx_04_02" , echo=TRUE, eval=TRUE}

df_vinho_tinto_treino$Predicao.Qualidade = predict(Modelo_01, newdata = subset(df_vinho_tinto_treino, select=c(qualidade, acidez_fixa,acidez_volatil,acido_citrico,acucar_residual,cloretos,densidade,fsd,grau_alcolico,PH,sulfatos,tsd)))

treino.corr = round(cor(df_vinho_tinto_treino$Predicao.Qualidade, df_vinho_tinto_treino$qualidade), 2)
treino.RMSE = round(sqrt(mean((df_vinho_tinto_treino$Predicao.Qualidade - df_vinho_tinto_treino$qualidade)^2)))
treino.MAE = round(mean(abs(df_vinho_tinto_treino$Predicao.Qualidade - df_vinho_tinto_treino$qualidade)))

c(treino.corr ^ 2, treino.RMSE, treino.MAE)

```

### Teste do Modelo

```{r chunk="idx_04_02" , echo=TRUE, eval=TRUE}

df_vinho_tinto_teste$Predicao.Qualidade = predict(Modelo_01, newdata = subset(df_vinho_tinto_teste, select=c(qualidade, acidez_fixa,acidez_volatil,acido_citrico,acucar_residual,cloretos,densidade,fsd,grau_alcolico,PH,sulfatos,tsd)))

teste.corr = round(cor(df_vinho_tinto_teste$Predicao.Qualidade, df_vinho_tinto_teste$qualidade), 2)
teste.RMSE = round(sqrt(mean((df_vinho_tinto_teste$Predicao.Qualidade - df_vinho_tinto_teste$qualidade)^2)))
teste.MAE = round(mean(abs(df_vinho_tinto_teste$Predicao.Qualidade - df_vinho_tinto_teste$qualidade)))

c(teste.corr ^ 2, teste.RMSE, teste.MAE)
```

### Relacionamento da Variável "qualidade"

```{r chunk="idx_04_02" , echo=TRUE, eval=TRUE}

par(mfrow=c(1,2))
plot(df_tinto$qualidade, df_tinto$grau_alcolico, main = "qualidade vs grau_alcolico", xlab="qualidade", ylab = "grau_alcolico", col = 3, pch = 1)
plot(df_tinto$qualidade, df_tinto$acidez_volatil, main = "qualidade vs acidez_volatil", xlab="qualidade", ylab = "acidez_volatil", col = 3, pch = 1)

par(mfrow=c(1,2))
plot(df_tinto$qualidade, df_tinto$acido_citrico, main = "qualidade vs acido_citrico", xlab="qualidade", ylab = "acido_citrico", col = 3, pch = 1)
plot(df_tinto$qualidade, df_tinto$sulfatos, main = "qualidade vs sulfatos", xlab="qualidade", ylab = "sulfatos", col = 3, pch = 1)
```

## Vinho Branco

### Separação dos Dados Treino e Teste

```{r chunk="idx_04_01" , echo=TRUE, eval=TRUE}
set.seed(1914) #seed relacionado ao ano de fundação do nosso amado e glorioso Palestra Itália

split <- sample.split(df_branco$qualidade, SplitRatio = 0.8)

#dividindo o dataset para treino e teste
df_vinho_branco_treino <- subset(df_branco, split == TRUE)
df_vinho_branco_teste  <- subset(df_branco, split == FALSE)
```

### Modelo Vinho Branco

```{r chunk="idx_04_05" , echo=TRUE, eval=TRUE}
Modelo_01 <- lm(qualidade~ acidez_fixa+acidez_volatil+acido_citrico+acucar_residual+cloretos+densidade+fsd+grau_alcolico+PH+sulfatos+tsd, 
                data = df_vinho_branco_treino)

precisao.branco.rlinear = summary(Modelo_01)$r.squared * 100

summary(Modelo_01)
plot(Modelo_01)
```

### Treino do Modelo

```{r chunk="idx_04_02" , echo=TRUE, eval=TRUE}

df_vinho_branco_treino$Predicao.Qualidade = predict(Modelo_01, newdata = subset(df_vinho_branco_treino, select=c(qualidade, acidez_fixa, acidez_volatil, acido_citrico, acucar_residual, cloretos, densidade, fsd, grau_alcolico, PH, sulfatos, tsd)))

treino.corr = round(cor(df_vinho_branco_treino$Predicao.Qualidade, df_vinho_branco_treino$qualidade), 2)
treino.RMSE = round(sqrt(mean((df_vinho_branco_treino$Predicao.Qualidade - df_vinho_branco_treino$qualidade)^2)))
treino.MAE = round(mean(abs(df_vinho_branco_treino$Predicao.Qualidade - df_vinho_branco_treino$qualidade)))

c(treino.corr ^ 2, treino.RMSE, treino.MAE)

```

### Teste do Modelo

```{r chunk="idx_04_02" , echo=TRUE, eval=TRUE}

df_vinho_branco_teste$Predicao.Qualidade = predict(Modelo_01, newdata = subset(df_vinho_branco_teste, select=c(qualidade, acidez_fixa, acidez_volatil, acido_citrico, acucar_residual, cloretos, densidade, fsd, grau_alcolico, PH, sulfatos, tsd)))

teste.corr = round(cor(df_vinho_branco_teste$Predicao.Qualidade, df_vinho_branco_teste$qualidade), 2)
teste.RMSE = round(sqrt(mean((df_vinho_branco_teste$Predicao.Qualidade - df_vinho_branco_teste$qualidade)^2)))
teste.MAE = round(mean(abs(df_vinho_branco_teste$Predicao.Qualidade - df_vinho_branco_teste$qualidade)))

c(teste.corr ^ 2, teste.RMSE, teste.MAE)
```

### Relacionamento da Variável "qualidade"

```{r chunk="idx_04_02" , echo=TRUE, eval=TRUE}
par(mfrow=c(1,2))
plot(df_branco$qualidade, df_branco$grau_alcolico, main = "qualidade vs grau_alcolico", xlab="qualidade", ylab = "grau_alcolico", col = 3, pch = 1)
plot(df_branco$qualidade, df_branco$acidez_volatil, main = "qualidade vs acidez_volatil", xlab="qualidade", ylab = "acidez_volatil", col = 3, pch = 1)

par(mfrow=c(1,2))
plot(df_branco$qualidade, df_branco$acido_citrico, main = "qualidade vs acido_citrico", xlab="qualidade", ylab = "acido_citrico", col = 3, pch = 1)
plot(df_branco$qualidade, df_branco$sulfatos, main = "qualidade vs sulfatos", xlab="qualidade", ylab = "sulfatos", col = 3, pch = 1)

par(mfrow=c(1,2))
plot(df_branco$qualidade, df_branco$acidez_fixa, main = "qualidade vs acidez_fixa", xlab="qualidade", ylab = "acidez_fixa", col = 3, pch = 1)
plot(df_branco$qualidade, df_branco$acucar_residual, main = "qualidade vs acucar_residual", xlab="qualidade", ylab = "acucar_residual", col = 3, pch = 1)

par(mfrow=c(1,2))
plot(df_branco$qualidade, df_branco$cloretos, main = "qualidade vs cloretos", xlab="qualidade", ylab = "cloretos", col = 3, pch = 1)
plot(df_branco$qualidade, df_branco$densidade, main = "qualidade vs densidade", xlab="qualidade", ylab = "densidade", col = 3, pch = 1)

par(mfrow=c(1,2))
plot(df_branco$qualidade, df_branco$fsd, main = "qualidade vs fsd", xlab="qualidade", ylab = "fsd", col = 3, pch = 1)
plot(df_branco$qualidade, df_branco$PH, main = "qualidade vs PH", xlab="qualidade", ylab = "PH", col = 3, pch = 1)

par(mfrow=c(1,2))
plot(df_branco$qualidade, df_branco$tsd, main = "qualidade vs tsd", xlab="qualidade", ylab = "tsd", col = 3, pch = 1)
```

# Árvore de Regressão

## Técnica

É muito similar a árvore de decisão, pois segue a mesma ideia: um conjunto de nós de DECISÃO/PERGUNTAS partindo de exemplos.

A única diferença é que a resposta é um número ao invés de uma categoria.

A obtenção de árvores de regressão usando o R é feita por meio da função
rpart, tal como nas árvores de decisão. 

## Vinho Tinto
```{r chunk="idx_05_01" , echo=TRUE, eval=TRUE}
RT_Modelo_01 <- rpart(qualidade ~ acidez_fixa+acidez_volatil+acido_citrico+acucar_residual+cloretos+densidade+fsd+grau_alcolico+PH+sulfatos+tsd, data = df_vinho_tinto_treino)
summary(RT_Modelo_01)
rpart.plot(RT_Modelo_01, digits = 9, fallen.leaves = TRUE, box.palette="RdBu", shadow.col="gray", nn=TRUE)

```



O Modelo apresentado acima teve um desepenho melhor que Regressão Linear Observamos que a variavel Grau Alcoolico é a variavel que possui o maior peso pois ela está no node mais alto da arvore.


### Análise da Qualidade do Modelo (Matriz de Confusão)

```{r chunk="idx_05_02" , echo=TRUE, eval=TRUE}

RT_preditor <- predict(RT_Modelo_01, newdata = df_vinho_tinto_teste)
RT_Valores_Corte <- as.factor(ifelse(RT_preditor > 6,1,0))
confusionMatrix(RT_Valores_Corte, df_vinho_tinto_teste$GrupoQualidade)

precisao.tinto.aregressao=confusionMatrix(RT_Valores_Corte,
                                         df_vinho_tinto_teste$GrupoQualidade)$overall["Accuracy"] * 100


```


## Vinho Branco

```{r chunk="idx_05_03" , echo=TRUE, eval=TRUE}
RT_Modelo_01 <- rpart(qualidade ~ acidez_fixa+acidez_volatil+acido_citrico+acucar_residual+cloretos+densidade+fsd+grau_alcolico+PH+sulfatos+tsd, data = df_vinho_branco_treino)
summary(RT_Modelo_01)
rpart.plot(RT_Modelo_01, digits = 9, fallen.leaves = TRUE, box.palette="RdBu", shadow.col="gray", nn=TRUE)

```

Nota se que para determinar a qualidade do vinho branco utiliza-se menos variáveis que para vinho tinto


```{r chunk="idx_05_04" , echo=TRUE, eval=TRUE}

RT_preditor <- predict(RT_Modelo_01, newdata = df_vinho_branco_teste)
RT_Valores_Corte <- as.factor(ifelse(RT_preditor > 6,1,0))
confusionMatrix(RT_Valores_Corte, df_vinho_branco_teste$GrupoQualidade)

precisao.branco.aregressao=confusionMatrix(RT_Valores_Corte,
                                           df_vinho_branco_teste$GrupoQualidade)$overall["Accuracy"] * 100

```

# Árvore de Decisão

## Técnica

É muito utilizada para aprendizagem indutiva e é extremamente prática.

O conhecimento da Árvore de Decisão será baseado em uma estrutura de árvore para assim podermos realizar decisão. Porém, caso não queira representar em estruturá de árvores, pode ser facilmente representada por regras "se/então". Pode-se utilizar tanto em problemas supervisionados quanto não supervisionados.

A árvore decisão também consegue descobrir quais são os atributos de maior importância para predição formando uma estrutura de nós. 

A base é a mesma da árvore de regressão.

Classe de algoritmos de aprendizado baseado na árvore de decisão: ID3("top-down"), C4.5 etc.

É importante ressaltar que uanto menor a árvore, melhor será a indução. Isso basicamente quer dizer que: caso fique grande, pode cair num problema de overfitting ("100% de acerto").

Outra coisa que precisa-se lembrar em uma Árvore de Decisão é a entropia, a qual diz o quanto um conjunto de dados aleatório está "impuro".
E sempre varia entre 0 e 1, de acordo com a proporção de +/- no conjunto. Vale lembrar que a entropia é importante para o cálculo de ganho de informação para a árvore.

A entropia (binária) é dada pela seguinte fórmula:

$$Entropia(S) = -\sum p_{+} log_{2}  p_{+} - p_{-} log_{2} p_{-}$$

onde:

S: coleção S contendo exemplos
p(+): proporção de exemplos positivos em S;
p(-): proporção de exemplos negativos em S

Referencia:
http://web.tecnico.ulisboa.pt/ana.freitas/bioinformatics.ath.cx/bioinformatics.ath.cx/indexf23d.html?id=199

## Vinho Tinto

```{r chunk="idx_06_01" , echo=TRUE, eval=TRUE}

DT_Modelo01 <-rpart(GrupoQualidade ~ acidez_fixa+acidez_volatil+acido_citrico+acucar_residual+cloretos+densidade+fsd+grau_alcolico+PH+sulfatos+tsd, data = df_vinho_tinto_treino)

summary(DT_Modelo01)
rpart.plot(DT_Modelo01, digits = 19, fallen.leaves = TRUE)

```

### Análise da Qualidade do Modelo

```{r chunk="idx_06_02" , echo=TRUE, eval=TRUE}
DT_Preditor <- as.data.frame(predict(DT_Modelo01, newdata = df_vinho_tinto_teste))
DT_Preditor$factor <- as.factor(ifelse(DT_Preditor[["1"]] > DT_Preditor[["0"]],1,0))
confusionMatrix(DT_Preditor$factor, df_vinho_tinto_teste$GrupoQualidade)

precisao.tinto.adecisao=confusionMatrix(DT_Preditor$factor,
                                        df_vinho_tinto_teste$GrupoQualidade)$overall["Accuracy"] * 100

```

## Vinho Branco

```{r chunk="idx_06_03" , echo=TRUE, eval=TRUE}

DT_Modelo01 <-rpart(GrupoQualidade ~ acidez_fixa+acidez_volatil+acido_citrico+acucar_residual+cloretos+densidade+fsd+grau_alcolico+PH+sulfatos+tsd, data = df_base_branco)

summary(DT_Modelo01)
rpart.plot(DT_Modelo01, digits = 19, fallen.leaves = TRUE)

```

```{r chunk="idx_06_04" , echo=TRUE, eval=TRUE}
DT_Preditor <- as.data.frame(predict(DT_Modelo01, newdata = df_vinho_branco_teste))
DT_Preditor$factor <- as.factor(ifelse(DT_Preditor[["1"]] > DT_Preditor[["0"]],1,0))
confusionMatrix(DT_Preditor$factor, df_vinho_branco_teste$GrupoQualidade)

precisao.branco.adecisao=confusionMatrix(DT_Preditor$factor,
                                         df_vinho_branco_teste$GrupoQualidade)$overall["Accuracy"] * 100

```

# Regressão Logística

## Técnica

A regressão logística é um modelo no qual classificamos na qual a variável dependente possuem valores binários (intervalos entre 0 e 1), ou seja, um ou o outro e as independentes podem ser categóricas ou não.

Este tipo de modelo lida muito bem com variáveis de entrada (independentes) de tipo categórica e possui um grau relativamente alto de confiabilidade.

Podemos dizer de modo geral que funciona como uma regressão linear, com exceção de que as variáveis dependentes devem ser categóricas e utiliza o método de máxima verossimilhança, ao invés dos mínimos quadrados como na regressão linear.

Como vimos, nosso dataset possui apenas dados numéricos, com exceção do tipo de vinho.

### Criando modelo Árvore de Regressão Logística da variável de saída "qualidade" com todas as variáveis de entrada


## Vinho Tinto

### Modelo Vinho Tinto

```{r chunk="idx_07_01" , echo=TRUE, eval=TRUE}

RL_Modelo01 <- glm(qualidade ~ acidez_fixa+acidez_volatil+acido_citrico+acucar_residual+cloretos+densidade+fsd+grau_alcolico+PH+sulfatos+tsd, data = df_vinho_tinto_treino)
summary(RL_Modelo01)
```

```{r chunk="idx_07_02" , echo=TRUE, eval=TRUE}

RL_Preditor <- predict.glm(RL_Modelo01, newdata = df_vinho_tinto_teste, type = 'response')
RL_Preditor_Corte <- as.factor(ifelse(RL_Preditor > 6,1,0))
confusionMatrix(RL_Preditor_Corte, df_vinho_tinto_teste$GrupoQualidade)

precisao.tinto.rlogistica=confusionMatrix(RL_Preditor_Corte,
                                          df_vinho_tinto_teste$GrupoQualidade)$overall["Accuracy"] * 100

```

## Vinho Branco

### Modelo Vinho Branco

```{r chunk="idx_07_03" , echo=TRUE, eval=TRUE}

RL_Modelo01 <- glm(qualidade ~ acidez_fixa+acidez_volatil+acido_citrico+acucar_residual+cloretos+densidade+fsd+grau_alcolico+PH+sulfatos+tsd, data = df_vinho_branco_treino)
summary(RL_Modelo01)
```

```{r chunk="idx_07_04" , echo=TRUE, eval=TRUE}

RL_Preditor <- predict.glm(RL_Modelo01, newdata = df_vinho_branco_teste, type = 'response')
RL_Preditor_Corte <- as.factor(ifelse(RL_Preditor > 6,1,0))
confusionMatrix(RL_Preditor_Corte, df_vinho_branco_teste$GrupoQualidade)

precisao.branco.rlogistica=confusionMatrix(RL_Preditor_Corte,
                                           df_vinho_branco_teste$GrupoQualidade)$overall["Accuracy"] * 100

```

# PCA

## Técnica

 Este algoritmo provê redução de dimensionalidade. Algumas vezes você tem uma grande quantidade de características, provavelmente muito correlacionadas entre si, e os modelos podem facilmente serem sobreajustados em um grande conjunto de dados. Neste cenário, aplica-se PCA.
 PCA calcula a projeção dos dados em algum vetor que maximize a variança dos dados e perca a menor quantidade de informação possível. Surpreendentemente, estes vetores são os autovetores da matriz de correlação das características de um conjunto de dados.

## Vinho Tinto

```{r chunk="idx_08_01" , echo=TRUE, eval=TRUE}

TintoPCA = prcomp(df_base_tinto[1:12] , scale. = TRUE)

summary(TintoPCA)

plot(1:12, TintoPCA$sdev^2, type = "b", xlab = "Componente",
     ylab = "Variância", main = "Vinhos tintos", sub = "Dados não normalizados previamente", pch = 20, cex.axis = 0.8, cex.lab = 0.8)

```

## Vinho Tinto base normalizada

```{r chunk="idx_08_02" , echo=TRUE, eval=TRUE}

TintoPCA = prcomp(df_tinto[1:12] , scale. = TRUE)

summary(TintoPCA)

plot(1:12, TintoPCA$sdev^2, type = "b", xlab = "Componente",
     ylab = "Variância", main = "Vinhos tintos", sub = "Dados normalizados previamente(Box Cox)", pch = 20, cex.axis = 0.8, cex.lab = 0.8)



```



## Vinho Branco

```{r chunk="idx_08_03" , echo=TRUE, eval=TRUE}

BrancoPCA = prcomp(df_base_branco[1:12], scale. = TRUE)

summary(BrancoPCA)

plot(1:12, BrancoPCA$sdev^2, type = "b", xlab = "Componente",
     ylab = "Variância", main = "Vinhos brancos", sub = "Dados não normalizados previamente", pch = 20, cex.axis = 0.8, cex.lab = 0.8)

```


## Vinho Branco base normalizada

```{r chunk="idx_08_04" , echo=TRUE, eval=TRUE}

BrancoPCA = prcomp(df_branco[1:12], scale. = TRUE)

summary(BrancoPCA)

plot(1:12, BrancoPCA$sdev^2, type = "b", xlab = "Componente",
     ylab = "Variância", main = "Vinhos brancos", sub = "Dados normalizados previamente(Box Cox)", pch = 20, cex.axis = 0.8, cex.lab = 0.8)

```

## Conclusão sobre PCA base normaliza vs base bruta

Esses 4 gráficos anteriores foram plotados para gerar a discussão sobre utilizar PCA com dados normalizados previamente ou não, para uma comparação de qual estratégia maximiza mais a variança dos dados.

O próprio algoritmo PCA trabalha na a variancia dos dados, mas usando os dados previamente tratados em relação a variancia com a função Box Cox, vemos que os gráficos dos dados previamente tratado são mais uniformes e é menor a discrepância de valors em certas componentes.


# K-Means

## Técnica

K-means é um algoritmo de aprendizagem não supervisionada que agrupa dados com base em sua similaridade. Aprendizagem não supervisionada significa que não há resultado a ser previsto, e o algoritmo apenas tenta encontrar padrões nos dados. No k-means, temos que especificar o número de grupos em que desejamos que os dados sejam agrupados. O algoritmo atribui aleatoriamente cada observação para um cluster, e encontra o centróide de cada cluster. Em seguida, o algoritmo segue em dois passos:

• Redistribui pontos de dados para o cluster cujo centróide é mais próximo.

• Calcula novo centróide para cada cluster.

Estes dois passos são repetidos até que a variação dentro do conjunto não possa mais ser reduzida. A variação dentro do cluster é calculada como a soma da distância euclidiana entre os pontos de dados e os os respectivos centróides de clusters.

## Vinho Tinto

```{r chunk="idx_09_01" , echo=TRUE, eval=TRUE}

Resultado <- kmeans(df_tinto[9:10], 5);
precisao.tinto.kmeans = Resultado$betweenss / Resultado$totss * 100
Resultado$size 

plot(df_tinto[9:10], col = Resultado$cluster, pch= 19)
points(Resultado$centers[9:10], col="RED", pch=7, cex=4)

```



```{r chunk="idx_09_02" , echo=TRUE, eval=TRUE}

Resultado <- kmeans(df_tinto[11:10], 5);
Resultado$size 

plot(df_tinto[11:10], col = Resultado$cluster, pch= 19)
points(Resultado$centers[11:10], col="RED", pch=7, cex=4)

```


```{r chunk="idx_09_03" , echo=TRUE, eval=TRUE}

Resultado <- kmeans(df_tinto[11:9], 5)
Resultado$size 

plot(df_tinto[11:9], col = Resultado$cluster, pch= 19)
points(Resultado$centers[11:9], col="RED", pch=7, cex=4)


```
### Algumas relações que conseguimos inferir com algortimo K-means para as variáveis alcoólico, sulfatos e pH na base vinho tinto

-Com Baixo teor alcoólico, temos baixos sulfatos
-Com baixos sulfatos, temos álcool elevado
-Com baixo pH, temos baixo teor alcoólico
-Com pH elevado, temos álcool alto
-Com pH elevado, temos baixos sulfatos
-Com baixo pH, temos altos sulfatos

é clara a relação do pH com álcool, o que segue as propriedades naturais conhecidas. Porém, a varíavel pH com sulfatos não tem um correlação forte, já que temos uma situação de alto e baixa quantidade de sulfatos na escala toda do pH.

sulfatos e álccol também não seguem uma relação proporcional.


## Vinho Branco

```{r chunk="idx_09_04" , echo=TRUE, eval=TRUE}

Resultado <- kmeans(df_branco[9:10], 5);
precisao.branco.kmeans = Resultado$betweenss / Resultado$totss * 100
Resultado$size

plot(df_branco[9:10], col = Resultado$cluster, pch= 19)
points(Resultado$centers[9:10], col="RED", pch=7, cex=4)

```


```{r chunk="idx_09_05" , echo=TRUE, eval=TRUE}

Resultado <- kmeans(df_branco[11:10], 5);
Resultado$size 

plot(df_branco[11:10], col = Resultado$cluster, pch= 19)
points(Resultado$centers[11:10], col="RED", pch=7, cex=4)

```


```{r chunk="idx_09_06" , echo=TRUE, eval=TRUE}

Resultado <- kmeans(df_branco[11:9], 5)
Resultado$size 

plot(df_branco[11:9], col = Resultado$cluster, pch= 19)
points(Resultado$centers[11:9], col="RED", pch=7, cex=4)

```

### Algumas relações que conseguimos inferir com algortimo K-means para as variáveis alcoólico, sulfatos e pH na base vinho branco

Para vinho branco é possível visualizar que para pH muito baixos não temos tanto sulfatos altos como na base vinho tintos e que há maior número de dados na faixa intermediária (-1 à 1, em ambos eixos), em relação a base vinho tintos. Ou seja, na base vinho tintos temos mais vinhos com sulfatos altos em relação aos vinhos brancos.


Na relação grau alcoólico e sulfatos, vemos que na base de vinhos brancos os dados são mais bem distribuídos, onde é notório que para grau alcoólico mais elevado temos tanto sultados baixos como altos.

Na comparação, das 3 variáveis juntas, é possível verificar que há maior quantidade de dados na faixa de pH baixo e grau alcoólico alto na base vinhos brancos do que vinhos tintos.


# Cluster Hierárquico

## Técnica

Separar um conjunto de objetos em grupos (clusters) de forma que os membros de qualquer grupo formado sejam os mais homogêneos possíveis com relação a algum critério (uso de medidas de distância)

Hierárquicos: identificam agrupamentos e o provável o n° g de grupos/clusters

### Separando Base Vinho

```{r chunk="idx_10_01" , echo=TRUE, eval=TRUE}

Resultado1 <- df_vinho_tinto_teste [12:10]

Resultado2 <- df_vinho_branco_teste [12:10]

```


## Vinho Tinto

### Distância Euclidiana

Métodos hierárquicos usam uma matriz de distância como uma entrada para o algoritmo de clustering. A escolha de uma métrica apropriada influenciará a forma dos aglomerados, pois alguns elementos podem estar próximos uns dos outros de acordo com uma distância e distantes de acordo com a outra.


```{r chunk="idx_10_02" , echo=TRUE, eval=TRUE}

d1 <-dist(Resultado1, method = "euclidean")
d1

```

### Dendrograma

O dendrograma é um diagrama de árvore que exibe os grupos formados por agrupamento de observações em cada passo e em seus níveis de similaridade. O nível de similaridade é medido ao longo do eixo vertical (alternativamente, você pode exibir o nível de distância) e as diferentes observações são listadas ao longo do eixo horizontal.

Usamos a distância euclidiana como uma entrada para o algoritmo de agrupamento (o critério de variação mínima de Ward minimiza a variação total dentro do cluster)

```{r chunk="idx_10_03" , echo=TRUE, eval=TRUE}

fit1 <-hclust(d1, method = "ward.D")
plot(fit1, hang = -1)
groups1 <- cutree(fit1, k=3)
rect.hclust(fit1, k=3, border="red")

```

### Resultado com Matriz de Confusão

O desempenho do cluster pode ser avaliado com o auxilio da matrix de confusão.


```{r chunk="idx_10_04" , echo=TRUE, eval=TRUE}

table(Resultado1[,1],groups1)

```

Esse Dendrograma foi criado usando uma partição de 3 agrupamentos para cada tipo de vinho, baseado em nivel de similaridade.

## Vinho Branco

### Distância Euclidiana

Métodos hierárquicos usam uma matriz de distância como uma entrada para o algoritmo de clustering. A escolha de uma métrica apropriada influenciará a forma dos aglomerados, pois alguns elementos podem estar próximos uns dos outros de acordo com uma distância e distantes de acordo com a outra.


```{r chunk="idx_10_02" , echo=TRUE, eval=TRUE}

d2 <-dist(Resultado2, method = "euclidean")
d2

```

### Dendrograma

O dendrograma é um diagrama de árvore que exibe os grupos formados por agrupamento de observações em cada passo e em seus níveis de similaridade. O nível de similaridade é medido ao longo do eixo vertical (alternativamente, você pode exibir o nível de distância) e as diferentes observações são listadas ao longo do eixo horizontal.

Usamos a distância euclidiana como uma entrada para o algoritmo de agrupamento (o critério de variação mínima de Ward minimiza a variação total dentro do cluster)

```{r chunk="idx_10_03" , echo=TRUE, eval=TRUE}

fit2 <-hclust(d2, method = "ward.D")
plot(fit2, hang = -1)
groups2 <- cutree(fit2, k=3)
rect.hclust(fit2, k=3, border="red")

```

### Resultado com Matriz de Confusão

O desempenho do cluster pode ser avaliado com o auxilio da matrix de confusão.


```{r chunk="idx_10_04" , echo=TRUE, eval=TRUE}

table(Resultado2[,1],groups2)

```

Esse Dendrograma foi criado usando uma partição de 3 agrupamentos para cada tipo de vinho, baseado em nivel de similaridade.





# Conclusão

## Precisão Prevista

### Vinho Tinto

```{r chunk="idx_09_06" , echo=TRUE, eval=TRUE}

precisao.tinto = data.frame("Técnica" = c("Regressão Linear",
                                     "Árvore Regressão",
                                     "Árvore Decisao",
                                     "Regressão Logística",
                                     "K-Means"),
                       "Acurácia" = c(as.numeric(precisao.tinto.rlinear),
                                      as.numeric(precisao.tinto.aregressao),
                                      as.numeric(precisao.tinto.adecisao),
                                      as.numeric(precisao.tinto.rlogistica),
                                      as.numeric(precisao.tinto.kmeans)), 
                       stringsAsFactors = FALSE)

ggplot(precisao.tinto, aes(x=Técnica, y=Acurácia, fill=Técnica)) +
  geom_bar(stat="identity") +
  scale_fill_hue(c = 40)

```

### Vinho Branco

```{r chunk="idx_09_06" , echo=TRUE, eval=TRUE}

precisao.branco = data.frame("Técnica" = c("Regressão Linear",
                                     "Árvore Regressão",
                                     "Árvore Decisão",
                                     "Regressão Logística",
                                     "K-Means"),
                       "Acurácia" = c(as.numeric(precisao.branco.rlinear),
                                      as.numeric(precisao.branco.aregressao),
                                      as.numeric(precisao.branco.adecisao),
                                      as.numeric(precisao.branco.rlogistica),
                                      as.numeric(precisao.branco.kmeans)), 
                       stringsAsFactors = FALSE)

ggplot(precisao.branco, aes(x=Técnica, y=Acurácia, fill=Técnica)) +
  geom_bar(stat="identity") +
  scale_fill_hue(c = 40)

```


## Análise Final

Segue abaixo o resultado final do Grupo E2GR referente a Análise de Qualidade dos Vinhos:

* Os vinhos devem ser analisados separadamente porque seus preditores divergem.

* Efetuamos uma higienização do dataset por causa das duplicidades.

* Efetuamos a normalização dos dados utilizando a Técnica Box Cox para alcançar os objetivos.

* Acreditamos que os algoritimos de Regressão Logística tendem a ser mais precisos.

* Motivado por conta do grupo classificador de determinância de qualidade se é bom, médio ou ruim.

* Entendemos que os melhores Vinhos Tinto para Importar são aqueles que possuem o grau_alcolico < 0.54 e sulfatos < 0.15 ou grau_alcolico >= 0.55 e grau_alcolico < 0.99 e sulfatos < 0.08 e pH >= 0.38 

* Entendemos que os melhores Vinhos Branco para Importar são aqueles que possuem o grau_alcolico < 0.34 e acidez_volatil >= -0.07 ou grau_alcolico >= 0.35 e grau_alcolico < 1.07 e fsd < -1.61

O algoritmo Kmeans foi importante para conseguir visualizar graficamente como algumas variavéis se comportam e se possuem forte relação, como foi possível ver com pH e grau alcoolico, em uma relação diretamente proporcional. A propriedade sulfato não influenciou diretamente nas variáveis álcool e pH, pois haviam dados sem forte tendência em toda escala das variáveis comparadas. 

Um ponto interessante na utilização do Kmeans foi para comparação de variáveis em bases diferentes, como na do vinho tinto e branco. Assim, foi possível verificar que há maior quantidade de dados(No de vinhos) na faixa de pH baixo e grau alcoólico alto na base vinhos brancos do que vinhos tintos. Sendo assim possível ter um maior número de escolhas de vinhos brancos nesse range para importadora.
Sendo possível inferir, também, que há mais químicos presentes no vinho branco abaixando o pH. 

 Legislação(Lei nº 7.678) que determina o máximo de 1,00 g/Lde sulfatos(K2SO4), com a visualização fácil através do histograma poderiámos sinalizar para importadora não comprar os vinhos com teor muito altos em ranges acima de 0.9.