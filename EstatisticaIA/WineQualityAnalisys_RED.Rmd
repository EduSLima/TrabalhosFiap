---
title: "ANALISE DO DATASET WINE QUALITY - VINHO TINTO"
output: html_notebook
---


Como analisado anteriormente, esta parte do trabalho visa realizar a análise do dataset WineQuality contudo será um estudo direcionado ao Vinho Tinto

```{r}

#criando um data set para vinho Tinto
vinho_tinto <-(subset(vinhos_ajustado, tipo=="RED"))

```




Mais uma vez iremos analisar as correlações existentes para o vinho Tinto, para isso iremos utilizar o metodo de speraman para analise de correlações em todas as observações de vinho tinto


```{r}

ggcorr(vinho_tinto, method = c("all.obs", "spearman"))

```

## Transformação de qualidade em variável categórica

Decidimos por classificar a nota da qualidade inicialmente em três grupos:
* Ruim: 0 ~ 5.99
* Regular: 6 ~ 7.99
* Bom: >= 8

Neste caso, poderíamos utilizar algoritmos supervisionados como o K-means pra predizer em qual categoria um vinho se encontra.

Porém, consideramos que isso não faria sentido para rodar os modelos não supervisionados.

Para rodar este modelo, decidimos criar a variável `GrupoQualidade`, sendo qualquer `qualidade` com valor maior ou superior a 6 é classificado como vinho "BOM". A variável `GrupoQualidade` será nossa variável dependente no caso.

```{r 10}

vinho_tinto$GrupoQualidade<- as.factor(ifelse(vinho_tinto$qualidade > 6,1,0))

```



## Modelo 1: Regressão Linear

### Teoria
  Regressão linear é uma equação para se estimar a condicional (valor esperado) de uma variável y, dados os valores de algumas outras variáveis x.

  Exemplo de regressão linear.
    A regressão, em geral, tem como objectivo tratar de um valor que não se consegue estimar inicialmente.

  A regressão linear é chamada "linear" porque se considera que a relação da resposta às variáveis é uma função linear de alguns parâmetros. Os modelos de regressão que não são uma função linear dos parâmetros se chamam modelos de regressão não-linear. Sendo uma das primeiras formas de análise regressiva a ser estudada rigorosamente, e usada extensamente em aplicações práticas. Isso acontece porque modelos que dependem de forma linear dos seus parâmetros desconhecidos, são mais fáceis de ajustar que os modelos não-lineares aos seus parâmetros, e porque as propriedades estatísticas dos estimadores resultantes são fáceis de determinar.[1]

### Formula 

  $$y_{i} = \alpha + \beta X_{i} + \varepsilon_{i}$$

onde:
  
$$y_{i}$$: Variável explicada (dependente); representa o que o modelo tentará prever
$$\alpha$$: É uma constante, que representa a interceptação da reta com o eixo vertical;
$$\beta$$: Representa a inclinação (coeficiente angular) em relação à variável explicativa;
$$X_{i}$$: Variável explicativa (independente);
$$\varepsilon _{i}}$$: Representa todos os factores residuais mais os possíveis erros de medição. O seu comportamento é aleatório, devido à natureza dos factores que encerra. Para que essa fórmula possa ser aplicada, os erros devem satisfazer determinadas hipóteses, que são: terem distribuição normal, com a mesma variância independentes e independentes da variável explicativa X, ou seja, i.i.d. (independentes e identicamente distribuídas).

fonte: https://pt.wikipedia.org/wiki/Regress%C3%A3o_linear



### Separando o dataset em treinamento/teste (80% / 20%)

```{r}
set.seed(7)
split <- sample.split(vinho_tinto$qualidade, SplitRatio = 0.8)

#dividindo o dataset para treino e teste
vt_treino <- subset(vinho_tinto, split == TRUE)
vt_teste  <- subset(vinho_tinto, split == FALSE)
```

Aplicando o modelo de regressão na base de treino:

```{r}


Modelo_01 <- lm(qualidade ~ acidez_fixa+acidez_volatil+acido_citrico+acucar_residual+cloretos+densidade+fsd+grau_alcolico+PH+sulfatos+tsd, data = vt_treino)
summary(Modelo_01)

```


### Análise da Qualidade do Modelo (Matriz de Confusão)

Vamos ver como o modelo se comporta utiliza todas as variáveis.

```{r}
  prediction_lm <- predict.lm(Modelo_01, newdata = vt_teste, type = 'response')
  prediction_lm_values <- as.factor(ifelse(prediction_lm > 6,1,0))
  confusionMatrix(prediction_lm_values, vt_teste$GrupoQualidade)
```



Percebemos aqui que temos uma acurácia de `79%` levando em consideração todas as variaveis disponives no modelo e que algumas delas não possuem forte correção, neste caso iremos diminuir o numero de variaveis para refazer o teste a fim de aumentar o nivel de acerto do modelo

```{r}
# selecionando variáveis por método automático

stepwise<-step(Modelo_01,direction="both")
 
stepwise
summary(stepwise)

```

Após realizado analise de Step_wise para determinar as melhores variáveis o resultado gerado nos mostra que as melhores variavies para se usar no modelo sao:
qualidade ~ acidez_volatil + cloretos + fsd + grau_alcolico + PH + sulfatos + tsd

```{r}

Modelo_02 <- lm(qualidade ~ acidez_volatil + cloretos + fsd + grau_alcolico + PH + sulfatos + tsd, data = vt_treino)
summary(Modelo_02)

```


```{r}

  prediction_lm_2 <- predict.lm(Modelo_02, newdata = vt_teste, type = 'response')
  prediction_lm_values_2 <- as.factor(ifelse(prediction_lm_2 > 6,1,0))
  confusionMatrix(prediction_lm_values_2, vt_teste$GrupoQualidade)

```

A conclusão que temos é que usando as variaveis sugeridas pelo metodo step tivemos um ganho de quase `1%`.


```{r}

qqnorm(residuals(Modelo_02), ylab="Resíduos",xlab="Quantis teóricos",main="")
qqline(residuals(Modelo_02))

shapiro.test(residuals(Modelo_02))

```

## Modelo 2: Árvore de Regressão

### Técnica

É muito similar a árvore de decisão, pois segue a mesma ideia: um conjunto de nós de DECISÃO/PERGUNTAS partindo de exemplos.

A única diferença é que a resposta é um número ao invés de uma categoria.

A obtenção de árvores de regressão usando o R é feita por meio da função
rpart, tal como nas árvores de decisão. 


### Criando modelo Árvore de Regressão com todas as variáveis (Qualidade: variável principal)

Neste modelo, temos como variável dependente o campo `Qualidade` e independente todo o restante de variáveis.

```{r 17}

#minermonico RT(regression tree) usado para distinguir os modelos aplicados

RT_Modelo_01 <- rpart(qualidade ~ acidez_fixa+acidez_volatil+acido_citrico+acucar_residual+cloretos+densidade+fsd+grau_alcolico+PH+sulfatos+tsd, data = vt_treino)
summary(RT_Modelo_01)
rpart.plot(RT_Modelo_01, digits = 9, fallen.leaves = TRUE, box.palette="RdBu", shadow.col="gray", nn=TRUE)
```

### Análise da Qualidade do Modelo (Matriz de Confusão)

```{r 18}

RT_preditor <- predict(RT_Modelo_01, newdata = vt_teste)
RT_Valores_Corte <- as.factor(ifelse(RT_preditor > 6,1,0))
confusionMatrix(RT_Valores_Corte, vt_teste$GrupoQualidade)

```

O Modelo apresentado acima teve um desepenho melhor que Regressão Linera obtendo `82%` de acuracia. Observamos que a variavel Grau Alcoolico é a variavel que possui o maior peso pois ela está no node mais alto da arvore.
