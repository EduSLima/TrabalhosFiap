---
title: "ANALISE DO DATASET WINE QUALITY - VINHO TINTO"
output: html_notebook
---


Como analisado anteriormente, esta parte do trabalho visa realizar a análise do dataset WineQuality contudo será um estudo direcionado ao Vinho Tinto

```{r}

#criando um data set para vinho Tinto
vinho_tinto <-(subset(vinhos_ajustado, tipo=="RED"))

```


Mais uma vez iremos analisar as correlações existentes para o vinho Tinto, para isso iremos utilizar o metodo de speraman para analise de correlações em todas as observações de vinho tinto


```{r}

ggcorr(vinho_tinto, method = c("all.obs", "spearman"))

```

## Transformação de qualidade em variável categórica

Decidimos por classificar a nota da qualidade inicialmente em três grupos:
* Ruim: 0 ~ 5.99
* Regular: 6 ~ 7.99
* Bom: >= 8

Neste caso, poderíamos utilizar algoritmos supervisionados como o K-means pra predizer em qual categoria um vinho se encontra.

Porém, consideramos que isso não faria sentido para rodar os modelos não supervisionados.

Para rodar este modelo, decidimos criar a variável `GrupoQualidade`, sendo qualquer `qualidade` com valor maior ou superior a 6 é classificado como vinho "BOM". A variável `GrupoQualidade` será nossa variável dependente no caso.

```{r 10}

vinho_tinto$GrupoQualidade<- as.factor(ifelse(vinho_tinto$qualidade > 6,1,0))

```



## Modelo 1: Regressão Linear

### Teoria
  Regressão linear é uma equação para se estimar a condicional (valor esperado) de uma variável y, dados os valores de algumas outras variáveis x.

  Exemplo de regressão linear.
    A regressão, em geral, tem como objectivo tratar de um valor que não se consegue estimar inicialmente.

  A regressão linear é chamada "linear" porque se considera que a relação da resposta às variáveis é uma função linear de alguns parâmetros. Os modelos de regressão que não são uma função linear dos parâmetros se chamam modelos de regressão não-linear. Sendo uma das primeiras formas de análise regressiva a ser estudada rigorosamente, e usada extensamente em aplicações práticas. Isso acontece porque modelos que dependem de forma linear dos seus parâmetros desconhecidos, são mais fáceis de ajustar que os modelos não-lineares aos seus parâmetros, e porque as propriedades estatísticas dos estimadores resultantes são fáceis de determinar.[1]

### Formula 

  $$y_{i} = \alpha + \beta X_{i} + \varepsilon_{i}$$

onde:
  
$$y_{i}$$: Variável explicada (dependente); representa o que o modelo tentará prever
$$\alpha$$: É uma constante, que representa a interceptação da reta com o eixo vertical;
$$\beta$$: Representa a inclinação (coeficiente angular) em relação à variável explicativa;
$$X_{i}$$: Variável explicativa (independente);
$$\varepsilon _{i}}$$: Representa todos os factores residuais mais os possíveis erros de medição. O seu comportamento é aleatório, devido à natureza dos factores que encerra. Para que essa fórmula possa ser aplicada, os erros devem satisfazer determinadas hipóteses, que são: terem distribuição normal, com a mesma variância independentes e independentes da variável explicativa X, ou seja, i.i.d. (independentes e identicamente distribuídas).

fonte: https://pt.wikipedia.org/wiki/Regress%C3%A3o_linear



### Separando o dataset em treinamento/teste (80% / 20%)

```{r}
set.seed(7)
split <- sample.split(vinho_tinto$qualidade, SplitRatio = 0.8)

#dividindo o dataset para treino e teste
vt_treino <- subset(vinho_tinto, split == TRUE)
vt_teste  <- subset(vinho_tinto, split == FALSE)
```

Aplicando o modelo de regressão na base de treino:

```{r}


Modelo_01 <- lm(qualidade ~ acidez_fixa+acidez_volatil+acido_citrico+acucar_residual+cloretos+densidade+fsd+grau_alcolico+PH+sulfatos+tsd, data = vt_treino)
summary(Modelo_01)

```


### Análise da Qualidade do Modelo (Matriz de Confusão)

Vamos ver como o modelo se comporta utiliza todas as variáveis.

```{r}
  prediction_lm <- predict.lm(Modelo_01, newdata = vt_teste, type = 'response')
  prediction_lm_values <- as.factor(ifelse(prediction_lm > 6,1,0))
  confusionMatrix(prediction_lm_values, vt_teste$GrupoQualidade)
```



Percebemos aqui que temos uma acurácia de `79%` levando em consideração todas as variaveis disponives no modelo e que algumas delas não possuem forte correção, neste caso iremos diminuir o numero de variaveis para refazer o teste a fim de aumentar o nivel de acerto do modelo

```{r}
# selecionando variáveis por método automático

stepwise<-step(Modelo_01,direction="both")
 
stepwise
summary(stepwise)

```

Após realizado analise de Step_wise para determinar as melhores variáveis o resultado gerado nos mostra que as melhores variavies para se usar no modelo sao:
qualidade ~ acidez_volatil + cloretos + fsd + grau_alcolico + PH + sulfatos + tsd

```{r}

Modelo_02 <- lm(qualidade ~ acidez_volatil + cloretos + fsd + grau_alcolico + PH + sulfatos + tsd, data = vt_treino)
summary(Modelo_02)

```


```{r}

  prediction_lm_2 <- predict.lm(Modelo_02, newdata = vt_teste, type = 'response')
  prediction_lm_values_2 <- as.factor(ifelse(prediction_lm_2 > 6,1,0))
  confusionMatrix(prediction_lm_values_2, vt_teste$GrupoQualidade)

```

A conclusão que temos é que usando as variaveis sugeridas pelo metodo step tivemos um ganho de quase `1%`.


```{r}

qqnorm(residuals(Modelo_02), ylab="Resíduos",xlab="Quantis teóricos",main="")
qqline(residuals(Modelo_02))

shapiro.test(residuals(Modelo_02))

```

## Modelo 2: Árvore de Regressão

### Técnica

É muito similar a árvore de decisão, pois segue a mesma ideia: um conjunto de nós de DECISÃO/PERGUNTAS partindo de exemplos.

A única diferença é que a resposta é um número ao invés de uma categoria.

A obtenção de árvores de regressão usando o R é feita por meio da função
rpart, tal como nas árvores de decisão. 


### Criando modelo Árvore de Regressão com todas as variáveis (Qualidade: variável principal)

Neste modelo, temos como variável dependente o campo `Qualidade` e independente todo o restante de variáveis.

```{r }

#minermonico RT(regression tree) usado para distinguir os modelos aplicados

RT_Modelo_01 <- rpart(qualidade ~ acidez_fixa+acidez_volatil+acido_citrico+acucar_residual+cloretos+densidade+fsd+grau_alcolico+PH+sulfatos+tsd, data = vt_treino)
summary(RT_Modelo_01)
rpart.plot(RT_Modelo_01, digits = 9, fallen.leaves = TRUE, box.palette="RdBu", shadow.col="gray", nn=TRUE)
```

### Análise da Qualidade do Modelo (Matriz de Confusão)

```{r }

RT_preditor <- predict(RT_Modelo_01, newdata = vt_teste)
RT_Valores_Corte <- as.factor(ifelse(RT_preditor > 6,1,0))
confusionMatrix(RT_Valores_Corte, vt_teste$GrupoQualidade)

```

O Modelo apresentado acima teve um desepenho melhor que Regressão Linera obtendo `82%` de acuracia. Observamos que a variavel Grau Alcoolico é a variavel que possui o maior peso pois ela está no node mais alto da arvore.



## Modelo 3: Árvore de Decisão

### Técnica

É muito utilizada para aprendizagem indutiva e é extremamente prática.

O conhecimento da Árvore de Decisão será baseado em uma estrutura de árvore para assim podermos realizar decisão. Porém, caso não queira representar em estruturá de árvores, pode ser facilmente representada por regras "se/então". Pode-se utilizar tanto em problemas supervisionados quanto não supervisionados.

A árvore decisão também consegue descobrir quais são os atributos de maior importância para predição formando uma estrutura de nós. 

A base é a mesma da árvore de regressão.

Classe de algoritmos de aprendizado baseado na árvore de decisão: ID3("top-down"), C4.5 etc.

É importante ressaltar que uanto menor a árvore, melhor será a indução. Isso basicamente quer dizer que: caso fique grande, pode cair num problema de overfitting ("100% de acerto").

Outra coisa que precisa-se lembrar em uma Árvore de Decisão é a entropia, a qual diz o quanto um conjunto de dados aleatório está "impuro".
E sempre varia entre 0 e 1, de acordo com a proporção de +/- no conjunto. Vale lembrar que a entropia é importante para o cálculo de ganho de informação para a árvore.

A entropia (binária) é dada pela seguinte fórmula:

$$Entropia(S) = -\sum p_{+} log_{2}  p_{+} - p_{-} log_{2} p_{-}$$

onde:

S: coleção S contendo exemplos
p(+): proporção de exemplos positivos em S;
p(-): proporção de exemplos negativos em S

Referencia:
http://web.tecnico.ulisboa.pt/ana.freitas/bioinformatics.ath.cx/bioinformatics.ath.cx/indexf23d.html?id=199


### Criando modelo Árvore de Decisão (Variável Principal: GrupoQualidade)

```{r}

DT_Modelo01 <-rpart(GrupoQualidade ~ acidez_fixa+acidez_volatil+acido_citrico+acucar_residual+cloretos+densidade+fsd+grau_alcolico+PH+sulfatos+tsd, data = vt_treino)

summary(DT_Modelo01)
rpart.plot(DT_Modelo01, digits = 19, fallen.leaves = TRUE)

```


### Análise da Qualidade do Modelo

```{r}
DT_Preditor <- as.data.frame(predict(DT_Modelo01, newdata = vt_teste))
DT_Preditor$factor <- as.factor(ifelse(DT_Preditor[["1"]] > DT_Preditor[["0"]],1,0))
confusionMatrix(DT_Preditor$factor, vt_teste$GrupoQualidade)
```

Até agora notamos que este foi o modelo que mais acertou com `87%` de acurácia, justamente por que a técnica é capaz de prever com maior exatidao quais as variaveis mais importantes para determinar a qualidade.


## Modelo 4:  Regressão Logística

### Técnica

A regressão logística é um modelo no qual classificamos na qual a variável dependente possuem valores binários (intervalos entre 0 e 1), ou seja, um ou o outro e as independentes podem ser categóricas ou não.

Este tipo de modelo lida muito bem com variáveis de entrada (independentes) de tipo categórica e possui um grau relativamente alto de confiabilidade.

Podemos dizer de modo geral que funciona como uma regressão linear, com exceção de que as variáveis dependentes devem ser categóricas e utiliza o método de máxima verossimilhança, ao invés dos mínimos quadrados como na regressão linear.

Como vimos, nosso dataset possui apenas dados numéricos, com exceção do tipo de vinho.

### Criando modelo Árvore de Regressão Logística com todas as variáveis (quality: variável principal)

```{r}
RL_Modelo01 <- glm(qualidade ~ acidez_fixa+acidez_volatil+acido_citrico+acucar_residual+cloretos+densidade+fsd+grau_alcolico+PH+sulfatos+tsd, data = vt_treino)
summary(RL_Modelo01)
```

### Análise da Qualidade do Modelo (Matriz de Confusão)

```{r}
RL_Preditor <- predict.glm(RL_Modelo01, newdata = vt_teste, type = 'response')
RL_Preditor_Corte <- as.factor(ifelse(RL_Preditor > 6,1,0))
confusionMatrix(RL_Preditor_Corte, vt_teste$GrupoQualidade)
```

Podemos verificar que na regressão logística a acurácia foi relativamente mais baixa (`79,6%` aproximadamente). Acreditamos que isso é porque a natureza de nosso problema envolve dados numéricos e a regressão logística se aplica melhor para casos em que se utilizam variáveis categóricas.


Redução de dimensionalidade PCA e Clusters

PCA

Colocar algum texto que faça menção a regra de funcionamento



```{r}

#necessario atribuir apenas valores numericos para o PCA neste caso iremos fazer um subset dos dados

TintoPCA = prcomp(vinho_tinto[1:12], scale. = TRUE)

summary(TintoPCA)


TintoPCA

plot(1:12, TintoPCA$sdev^2, type = "b", xlab = "Componente",
     ylab = "Variância", pch = 20, cex.axis = 0.8, cex.lab = 0.8)



```


De acordo com o Grafico 7 compoentes explicam a maior quantidade de dados

```{r}
biplot(TintoPCA)
```





```{r}

cumpro <- cumsum(TintoPCA$sdev^2 / sum(TintoPCA$sdev^2))
plot(cumpro[0:15], xlab = "PC #", ylab = "", main = "Variancia Cumulativa")
abline(v = 7, col="blue", lty=5)
abline(h = 0.88759, col="blue", lty=5)
legend("topleft", legend=c("Cut-off @ PC6"),
       col=c("blue"), lty=5, cex=0.6)

```


o Grafico acima demonstra que com 7 componentes conseguimos explicar quase 90% da variancia dos dados



```{r}

mat_tinto <-cor(vinho_tinto[1:12])

corrplot::corrplot(mat_tinto, method="circle", order="hclust")
```